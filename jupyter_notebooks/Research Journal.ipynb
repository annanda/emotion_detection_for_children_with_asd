{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection System using AVEC 2018\n",
    "\n",
    "## I mapped valence and arousasal into the four emotion zones using this reference: \n",
    "```\n",
    "    @article{tan2019automatic,\n",
    "  title={Automatic music mood recognition using Russellâ€™s twodimensional valence-arousal space from audio and lyrical data as classified using SVM and Na{\\\"\\i}ve Bayes},\n",
    " author={Tan, KR and Villarino, ML and Maderazo, C},\n",
    "  journal={MS\\&E},\n",
    "  volume={482},\n",
    "  number={1},\n",
    "  pages={012019},\n",
    "  year={2019}\n",
    "} ```\n",
    "### Two dimensional valence-arousal space\n",
    "![alt text](valence_arousal.png \"Two dimensional valence-arousal space\")\n",
    "\n",
    "### I took some decisions when we find values over orthogonal axis \n",
    "1. the point (0,0) is green\n",
    "2. the point (0,1) is yellow\n",
    "3. the point (1,0) is green\n",
    "4. the point (0,-1) is blue\n",
    "5. the point (-1,0) is red \n",
    "\n",
    "Following the same logic, I defined that:\n",
    "1. when the values rest on the valence axis (arousal value equals 0), \n",
    "  * If valence value is positive (valence greater than 0): green class \n",
    "  * Else (valence less than 0): red class \n",
    "\n",
    "\n",
    "2. when the values rest on the arousal axis (valence value equals 0), \n",
    "  * If arousal value is positive (arousal greater than 0): yellow class \n",
    "  * Else (arousal less than 0): blue class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path_annotation_emotions = '/Users/user/PycharmProjects/emotion_detection_system/labels/emotion_zones/emotion_names'\n",
    "files = glob.glob(f\"{path_annotation_emotions}/*.csv\")\n",
    "train_count = {'yellow': 0, 'red': 0, 'blue': 0, 'green': 0}\n",
    "dev_count = {'yellow': 0, 'red': 0, 'blue': 0, 'green': 0}\n",
    "for file in files:\n",
    "    df_train = pd.read_csv(file)\n",
    "    df_train['emotion_zone'].value_counts()\n",
    "    count_emotions = df_train['emotion_zone'].value_counts()\n",
    "    if 'train' in file:\n",
    "        train_count['yellow'] += count_emotions['yellow']\n",
    "        train_count['red'] += count_emotions['red']\n",
    "        train_count['blue'] += count_emotions['blue']\n",
    "        train_count['green'] += count_emotions['green']\n",
    "    elif 'dev' in file:\n",
    "        dev_count['yellow'] += count_emotions['yellow']\n",
    "        dev_count['red'] += count_emotions['red']\n",
    "        dev_count['blue'] += count_emotions['blue']\n",
    "        dev_count['green'] += count_emotions['green']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then computed statistics to find emotion zones distribution into train and dev datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution within all train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "keys = train_count.keys()\n",
    "values = train_count.values()\n",
    "\n",
    "plt.bar(keys, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution within all dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = dev_count.keys()\n",
    "values = dev_count.values()\n",
    "\n",
    "plt.bar(keys, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations: \n",
    "1. both datasets follow the same distribution of classes. \n",
    "2. the classes are very unbalanced. With ```red``` being the one with less examples. \n",
    "\n",
    "## Problem: Defining what zero means \n",
    "How much close to zero can we consider a value as zero? This might make a lot of difference on the class distribution. e.g. 0.01 should be seen as zero? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a baseline model (Dummy model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the data distribution of test set (made of 20% of dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_test_count = y_test.value_counts()\n",
    "\n",
    "\n",
    "test_count = {}\n",
    "test_count['yellow'] = y_test_count['yellow']\n",
    "test_count['red'] = y_test_count['red'] \n",
    "test_count['blue'] = y_test_count['blue'] \n",
    "test_count['green'] = y_test_count['green'] \n",
    "\n",
    "keys = test_count.keys()\n",
    "values = test_count.values()\n",
    "\n",
    "plt.bar(keys, values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model - most frequent prediction always\n",
    "\n",
    "### 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42979053529868116"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_VIDEO_FOLDER = '/Users/user/PycharmProjects/emotion_detection_system/dataset/video'\n",
    "\n",
    "x = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_train.csv').iloc[:, 2:-1]\n",
    "y = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_train.csv')['emotion_zone']\n",
    "\n",
    "x_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_dev.csv').iloc[:, 2:-1]\n",
    "y_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_dev.csv')['emotion_zone']\n",
    "x_dev, x_test = train_test_split(x_dev_dataset, test_size=0.2)\n",
    "y_dev, y_test = train_test_split(y_dev_dataset, test_size=0.2)\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "# dummy_clf = DummyClassifier(strategy=\"uniform\")\n",
    "dummy_clf.fit(x, y)\n",
    "predictions = dummy_clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0, 230,   0],\n",
       "       [  0,   0, 461,   0],\n",
       "       [  0,   0, 554,   0],\n",
       "       [  0,   0,  44,   0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions, labels=[\"blue\", \"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model - uniform random prediction\n",
    "### 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24903025601241272"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_VIDEO_FOLDER = '/Users/user/PycharmProjects/emotion_detection_system/dataset/video'\n",
    "\n",
    "x = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_train.csv').iloc[:, 2:-1]\n",
    "y = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_train.csv')['emotion_zone']\n",
    "\n",
    "x_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_dev.csv').iloc[:, 2:-1]\n",
    "y_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/AU_dev.csv')['emotion_zone']\n",
    "x_dev, x_test = train_test_split(x_dev_dataset, test_size=0.2)\n",
    "y_dev, y_test = train_test_split(y_dev_dataset, test_size=0.2)\n",
    "dummy_clf = DummyClassifier(strategy=\"uniform\")\n",
    "dummy_clf.fit(x, y)\n",
    "predictions = dummy_clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61,  62,  57,  63],\n",
       "       [121, 104, 110, 120],\n",
       "       [132, 128, 148, 140],\n",
       "       [  8,  19,   8,   8]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions, labels=[\"blue\", \"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ED model - Using SVM - Video input + AU features\n",
    "1. Only Video modality \n",
    "2. Only AU features \n",
    "\n",
    "### 1. Accuracy\n",
    "\n",
    "The accuracy obtained is greater than a random guess and less than always guessing on the most frequent in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model: 0.39332816136539955\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(x, y)\n",
    "\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy of SVM model: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix\n",
    "\n",
    "As expected, the class more corrected predicted is ```yellow```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9,  56, 169,   5],\n",
       "       [ 24,  89, 340,   8],\n",
       "       [ 25, 101, 409,  11],\n",
       "       [  3,   4,  36,   0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions, labels=[\"blue\", \"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ED model - Using SVM - Video input + appearance features\n",
    "1. Only Video modality \n",
    "2. Only appearance features \n",
    "\n",
    "### 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model: 0.42096774193548386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_VIDEO_FOLDER = '/Users/user/PycharmProjects/emotion_detection_system/dataset/video'\n",
    "\n",
    "def run_model_one_feature(video_feature, model):\n",
    "    x = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv').iloc[:, 2:-1]\n",
    "    y = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv')['emotion_zone']\n",
    "    x_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv').iloc[:, 2:-1]\n",
    "    y_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv')['emotion_zone']\n",
    "    x_dev, x_test = train_test_split(x_dev_dataset, test_size=0.2)\n",
    "    y_dev, y_test = train_test_split(y_dev_dataset, test_size=0.2)\n",
    "\n",
    "    if model == 'SVM':\n",
    "        clf = svm.SVC()\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    predictions = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f'Accuracy of SVM model: {accuracy}')\n",
    "\n",
    "    return predictions, y_test\n",
    "\n",
    "predictions, y_test = run_model_one_feature('appearance', 'SVM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,  34, 160,   0],\n",
       "       [ 12,  68, 381,   0],\n",
       "       [ 14,  76, 448,   0],\n",
       "       [  2,   4,  35,   0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions, labels=[\"blue\", \"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ED model - Using SVM - Video input + BoVW features\n",
    "1. Only Video modality \n",
    "2. Only BoVW features \n",
    "\n",
    "### 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model: 0.3809171597633136\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_VIDEO_FOLDER = '/Users/user/PycharmProjects/emotion_detection_system/dataset/video'\n",
    "\n",
    "def run_model_one_feature(video_feature, model):\n",
    "    x = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv').iloc[:, 2:-1]\n",
    "    y = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv')['emotion_zone']\n",
    "    x_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv').iloc[:, 2:-1]\n",
    "    y_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv')['emotion_zone']\n",
    "    x_dev, x_test = train_test_split(x_dev_dataset, test_size=0.2)\n",
    "    y_dev, y_test = train_test_split(y_dev_dataset, test_size=0.2)\n",
    "\n",
    "    if model == 'SVM':\n",
    "        clf = svm.SVC()\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    predictions = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f'Accuracy of SVM model: {accuracy}')\n",
    "\n",
    "    return predictions, y_test\n",
    "\n",
    "predictions, y_test = run_model_one_feature('BoVW', 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4,  59, 228,   1],\n",
       "       [ 11, 125, 358,   2],\n",
       "       [  7, 136, 386,   1],\n",
       "       [  0,   8,  26,   0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions, labels=[\"blue\", \"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ED model - Using SVM - Video input + (AU + appearance) features\n",
    "\n",
    "1. Only Video modality \n",
    "2. AU and appearance features \n",
    "\n",
    "### 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model: 0.43842364532019706\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "import functools\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_VIDEO_FOLDER = '/Users/user/PycharmProjects/emotion_detection_system/dataset/video'\n",
    "\n",
    "def concatenate_video_au_files(dataset_type, features_type):\n",
    "    \"\"\"\n",
    "    dataset_type must be 'train' or 'dev'\n",
    "    \"\"\"\n",
    "    files = glob.glob(f'{DATASET_VIDEO_FOLDER}/{features_type}/{dataset_type}_*.csv')\n",
    "\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "\n",
    "    concated = pd.concat(dfs)\n",
    "    concated.to_csv(f'{DATASET_VIDEO_FOLDER}/{features_type}_{dataset_type}.csv')\n",
    "\n",
    "def concatenate_different_features_type_dataset(dataset_type, features_type_list):\n",
    "    \"\"\"\n",
    "    dataset_type must be 'train' or 'dev'\n",
    "    \"\"\"\n",
    "    files = glob.glob(f'{DATASET_VIDEO_FOLDER}/{features_type_list[0]}/{dataset_type}_*.csv')\n",
    "\n",
    "    for file in files:\n",
    "        dfs = []\n",
    "        file_name = file.split('/')[-1]\n",
    "        df = pd.read_csv(file).iloc[:, :-1]\n",
    "        other_df_path = f'{DATASET_VIDEO_FOLDER}/{features_type_list[1]}/{file_name}'\n",
    "        other_df = pd.read_csv(other_df_path)\n",
    "        dfs.append(df)\n",
    "        dfs.append(other_df)\n",
    "        merged = functools.reduce(lambda df1, df2: pd.merge(df1, df2, on='frametime', how='inner'), dfs)\n",
    "        # print(merged)\n",
    "        merged.to_csv(f'{DATASET_VIDEO_FOLDER}/temp/{file_name}', index=False)\n",
    "\n",
    "def producing_more_than_one_features_type(feature_type_lst):\n",
    "    concatenate_different_features_type_dataset('train', feature_type_lst)\n",
    "    concatenate_different_features_type_dataset('dev', feature_type_lst)\n",
    "    concatenate_video_au_files('train', 'temp')\n",
    "    concatenate_video_au_files('dev', 'temp')\n",
    "\n",
    "def run_model_one_feature_type(video_feature, model):\n",
    "    x = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv').iloc[:, 2:-1]\n",
    "    y = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv')['emotion_zone']\n",
    "    x_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv').iloc[:, 2:-1]\n",
    "    y_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv')['emotion_zone']\n",
    "    x_dev, x_test = train_test_split(x_dev_dataset, test_size=0.2)\n",
    "    y_dev, y_test = train_test_split(y_dev_dataset, test_size=0.2)\n",
    "\n",
    "    if model == 'SVM':\n",
    "        clf = svm.SVC()\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    predictions = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f'Accuracy of SVM model: {accuracy}')\n",
    "    return predictions, y_test \n",
    "\n",
    "\n",
    "def run_model_more_than_one_feature_type(feature_type_list, model):\n",
    "    producing_more_than_one_features_type(feature_type_list)\n",
    "    predictions, y_test  = run_model_one_feature_type('temp', model)\n",
    "    return predictions, y_test \n",
    "\n",
    "\n",
    "feature_type_list = ['AU', 'appearance']\n",
    "predictions, y_test = run_model_more_than_one_feature_type(feature_type_list, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8,  50, 164,   1],\n",
       "       [ 19,  71, 308,   1],\n",
       "       [ 19,  87, 455,   0],\n",
       "       [  1,   8,  26,   0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions, labels=[\"blue\", \"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ED model - Using SVM - Video input + (AU + appearance + BoVW) features\n",
    "\n",
    "1. Only Video modality \n",
    "2. AU, appearance and BoVW features \n",
    "\n",
    "### 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model: 0.4006568144499179\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "import functools\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_VIDEO_FOLDER = '/Users/user/PycharmProjects/emotion_detection_system/dataset/video'\n",
    "\n",
    "def concatenate_video_au_files(dataset_type, features_type):\n",
    "    \"\"\"\n",
    "    dataset_type must be 'train' or 'dev'\n",
    "    \"\"\"\n",
    "    files = glob.glob(f'{DATASET_VIDEO_FOLDER}/{features_type}/{dataset_type}_*.csv')\n",
    "\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "\n",
    "    concated = pd.concat(dfs)\n",
    "    concated.to_csv(f'{DATASET_VIDEO_FOLDER}/{features_type}_{dataset_type}.csv')\n",
    "\n",
    "def concatenate_different_features_type_dataset(dataset_type, features_type_list):\n",
    "    \"\"\"\n",
    "    dataset_type must be 'train' or 'dev'\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(f'{DATASET_VIDEO_FOLDER}/{features_type_list[0]}/{dataset_type}_*.csv')\n",
    "\n",
    "    for file in files[:]:\n",
    "        dfs = []\n",
    "        file_name = file.split('/')[-1]\n",
    "        # df = pd.read_csv(file).iloc[:, :-1]\n",
    "        # dfs.append(df)\n",
    "        for i, feature_type in enumerate(features_type_list):\n",
    "            if i < len(features_type_list) - 1:\n",
    "                other_df_path = f'{DATASET_VIDEO_FOLDER}/{features_type_list[i]}/{file_name}'\n",
    "                other_df = pd.read_csv(other_df_path).iloc[:, :-1]\n",
    "                dfs.append(other_df)\n",
    "            else:\n",
    "                last_df_path = f'{DATASET_VIDEO_FOLDER}/{features_type_list[-1]}/{file_name}'\n",
    "                last_df_path = pd.read_csv(last_df_path)\n",
    "                dfs.append(last_df_path)\n",
    "\n",
    "        merged = functools.reduce(lambda df1, df2: pd.merge(df1, df2, on='frametime', how='inner'), dfs)\n",
    "        # print(merged)\n",
    "        merged.to_csv(f'{DATASET_VIDEO_FOLDER}/temp/{file_name}', index=False)\n",
    "\n",
    "def producing_more_than_one_features_type(feature_type_lst):\n",
    "    concatenate_different_features_type_dataset('train', feature_type_lst)\n",
    "    concatenate_different_features_type_dataset('dev', feature_type_lst)\n",
    "    concatenate_video_au_files('train', 'temp')\n",
    "    concatenate_video_au_files('dev', 'temp')\n",
    "\n",
    "def run_model_one_feature_type(video_feature, model):\n",
    "    x = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv').iloc[:, 2:-1]\n",
    "    y = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_train.csv')['emotion_zone']\n",
    "    x_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv').iloc[:, 2:-1]\n",
    "    y_dev_dataset = pd.read_csv(f'{DATASET_VIDEO_FOLDER}/{video_feature}_dev.csv')['emotion_zone']\n",
    "    x_dev, x_test = train_test_split(x_dev_dataset, test_size=0.2)\n",
    "    y_dev, y_test = train_test_split(y_dev_dataset, test_size=0.2)\n",
    "\n",
    "    if model == 'SVM':\n",
    "        clf = svm.SVC()\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    predictions = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f'Accuracy of SVM model: {accuracy}')\n",
    "    return predictions, y_test \n",
    "\n",
    "\n",
    "def run_model_more_than_one_feature_type(feature_type_list, model):\n",
    "    producing_more_than_one_features_type(feature_type_list)\n",
    "    predictions, y_test  = run_model_one_feature_type('temp', model)\n",
    "    return predictions, y_test \n",
    "\n",
    "\n",
    "feature_type_list = ['AU', 'appearance', 'BoVW']\n",
    "predictions, y_test = run_model_more_than_one_feature_type(feature_type_list, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3,  46, 167,   0],\n",
       "       [  9,  94, 355,   0],\n",
       "       [ 14, 100, 391,   0],\n",
       "       [  0,   7,  32,   0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions, labels=[\"blue\", \"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with video (AU + appearance + BoVW features) - SVM \n",
    "\n",
    "#### Accuracy of SVM model: 0.4302134646962233\n",
    "\n",
    "#### Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  6  42 167   0]\n",
    " [ 15 108 326   0]\n",
    " [ 17  92 410   0]\n",
    " [  1   9  25   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with video (AU  + BoVW features) - SVM \n",
    "\n",
    "#### Accuracy of SVM model: 0.395655546935609\n",
    "\n",
    "#### Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    " \n",
    "```\n",
    "[[  9  63 191   1]\n",
    " [ 14 117 317   0]\n",
    " [ 12 141 384   2]\n",
    " [  4   6  28   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with video (geometric features) - SVM \n",
    "\n",
    "#### Accuracy of SVM model: 0.4324960753532182\n",
    "\n",
    "#### Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    " \n",
    "```\n",
    "[[  0   5 232   0]\n",
    " [  0   8 433   0]\n",
    " [  0   6 543   0]\n",
    " [  0   3  44   0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with video (AU + appearance + BoVW + geometric features) - SVM \n",
    "\n",
    "#### Accuracy of SVM model: 0.4324960753532182\n",
    "\n",
    "#### Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    " \n",
    "```\n",
    "[[  0   4 208   0]\n",
    " [  0   8 422   0]\n",
    " [  0  11 530   0]\n",
    " [  0   0  35   0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with video (AU + geometric features) - SVM \n",
    "\n",
    "#### Accuracy of SVM model: 0.41626409017713367\n",
    "\n",
    "#### Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    " \n",
    "```\n",
    "[[  0   5 222   0]\n",
    " [  0   5 454   0]\n",
    " [  0   7 512   0]\n",
    " [  0   1  36   0]]\n",
    "```\n",
    "\n",
    "Accuracy of SVM model for video modality using ['AU', 'geometric'] as features: 0.4331723027375201\n",
    "```\n",
    "[[  0   7 213   0]\n",
    " [  0  11 432   0]\n",
    " [  0   8 527   0]\n",
    " [  0   1  43   0]]\n",
    " ```\n",
    " \n",
    " Accuracy of SVM model for video modality using ['AU', 'geometric'] as features: 0.4420289855072464\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  0   5 219   0]\n",
    " [  0  12 416   0]\n",
    " [  0  10 537   0]\n",
    " [  0   1  42   0]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with AUDIO (BoAW) - SVM \n",
    "\n",
    "#### Accuracy of SVM model: 0.4066666666666667\n",
    "\n",
    "#### Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    " \n",
    "```\n",
    "[[  5  78 164   0]\n",
    " [  4 166 327   0]\n",
    " [  6 179 378   0]\n",
    " [  0  18  25   0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13/07/2020\n",
    "# Regenerating all results: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': True, 'appearance': False, 'BoVW': False, 'geometric': False}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "\n",
    "Accuracy: 0.39875872769588827\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  4  67 195   0]\n",
    " [ 34  95 318   8]\n",
    " [ 30  76 414  14]\n",
    " [  0   5  28   1]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': False, 'appearance': True, 'BoVW': False, 'geometric': False}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.3758064516129032\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  5  45 177   0]\n",
    " [ 17  63 381   0]\n",
    " [ 14 101 398   0]\n",
    " [  4   7  28   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': False, 'appearance': False, 'BoVW': True, 'geometric': False}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.40458579881656803\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  8  64 190   3]\n",
    " [  6 115 360   0]\n",
    " [ 12 131 424   2]\n",
    " [  0  10  27   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': False, 'appearance': False, 'BoVW': False, 'geometric': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.41836734693877553\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  0  10 238   0]\n",
    " [  0   1 435   0]\n",
    " [  0   9 532   0]\n",
    " [  0   1  48   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': True, 'appearance': True, 'BoVW': True, 'geometric': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.4236453201970443\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  0   6 215   0]\n",
    " [  0   8 430   0]\n",
    " [  0   7 508   0]\n",
    " [  0   1  43   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'audio': {'features_type': {'BoAW': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.5044444444444445\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  4 106 149   0]\n",
    " [  3 255 275   1]\n",
    " [  5  92 422   0]\n",
    " [  0  10  28   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'audio': {'features_type': {'BoAW': False, 'DeepSpectrum': True, 'eGeMAPSfunct': False}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.4703703703703704\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  0 137 130   0]\n",
    " [  0 227 269   0]\n",
    " [  0 133 408   0]\n",
    " [  0  14  32   0]]```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': True, 'appearance': True, 'BoVW': True, 'geometric': True}, 'model': 'SVM'}, 'audio': {'features_type': {'BoAW': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.4734848484848485\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  1  16  39   0]\n",
    " [  0  21  63   0]\n",
    " [  0  13 103   0]\n",
    " [  0   0   8   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': False, 'appearance': False, 'BoVW': False, 'geometric': True}, 'model': 'SVM'}, 'audio': {'features_type': {'BoAW': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.4461538461538462\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[ 0  9 44  0]\n",
    " [ 0 20 71  0]\n",
    " [ 0 11 96  0]\n",
    " [ 0  3  6  0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that when using multimodal (the two results above), the number of examples for testset is way smaller (the sum of all numbers of the confusion matrix). \n",
    "\n",
    "This happens because, in the moment, the multimodality is a random selection of one of the two possible prediction modality - one coming from video and another from audio. \n",
    "To do this, the test dataset is the intersection of the frametime with avaliable features of video and audio. \n",
    "\n",
    "Since we do not have valid features for each frametime in the dataset for each modality, the intersection of the two modalities generates a testdataset smaller than using only one modality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can be done? \n",
    "\n",
    "I can include into the testset examples of when I have the prediction of only one modality available. \n",
    "So instead of being the intersection, it will be the union of the two modalitites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': True, 'appearance': False, 'BoVW': False, 'geometric': False}, 'model': 'SVM'}, 'audio': {'features_type': {'BoAW': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.45867768595041325\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[ 0 12 28  0]\n",
    " [ 4 28 58  1]\n",
    " [ 5 12 83  3]\n",
    " [ 0  3  5  0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset\n",
    "## 1. First method: undersampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using balanced training set of total of 1392 examples. \n",
    "348 examples of each class\n",
    "\n",
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': True, 'appearance': False, 'BoVW': False, 'geometric': False}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.2699767261442979\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[ 45 101  68  41]\n",
    " [118 121 117 111]\n",
    " [116 106 181 126]\n",
    " [  5  10  22   1]]```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using balanced training set of total of 1540 examples. \n",
    "385 examples of each class\n",
    "\n",
    "## Processing the input: {'modalities': {'audio': {'features_type': {'BoAW': False, 'DeepSpectrum': False, 'eGeMAPSfunct': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.2952167414050822\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[ 83 117  12  27]\n",
    " [150 265  31  42]\n",
    " [140 299  47  78]\n",
    " [ 15  29   3   0]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using balanced training set of total of 1588 examples. \n",
    "397 examples of each class\n",
    "\n",
    "## Processing the input: {'modalities': {'audio': {'features_type': {'BoAW': True, 'DeepSpectrum': True, 'eGeMAPSfunct': False}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.3711111111111111\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[ 79 121  36  27]\n",
    " [133 201 117  59]\n",
    " [ 68 161 215  94]\n",
    " [  3  15  15   6]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result using two modalities and processing the union of examples of each modality, i.e. if some modality is unavailable, the result of the other modality is used. If both modality are available, we radomly select one of them to be the predicted result. \n",
    "\n",
    "\n",
    "## VIDEO\n",
    "Using balanced training set of total of 1392 examples. \n",
    "348 examples of each class\n",
    "\n",
    "## AUDIO\n",
    "Using balanced training set of total of 1540 examples. \n",
    "385 examples of each class\n",
    "\n",
    "## Processing the input: {'modalities': {'video': {'features_type': {'AU': True, 'appearance': False, 'BoVW': False, 'geometric': False}, 'model': 'SVM'}, 'audio': {'features_type': {'BoAW': False, 'DeepSpectrum': False, 'eGeMAPSfunct': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "\n",
    "Accuracy: 0.627712219261515\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[ 35  92  86  50]\n",
    " [111 136 130  98]\n",
    " [133 117 136 138]\n",
    " [  4  14   5   4]]```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: The audio feature type DeepSpectrum cannot be used with other audio features, because they do note share the same timeframes. The annotators created this feature using frametimes of 400ms but starting from 1.0 not 0.0 (as the other features). We still have annotation for those frametimes, thus DeepSpectrum feature works alone or together with video features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using balanced training set of total of 908 examples. \n",
    "227 examples of each class\n",
    "\n",
    "## Processing the input: {'modalities': {'physio': {'features_type': {'HRHRV': True}, 'model': 'SVM'}}, 'fusion_type': 'late_fusion'}\n",
    "\n",
    "Accuracy: 0.3484646194926569\n",
    "\n",
    "Confusion matrix: labels=[\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "```\n",
    "[[  8  40  92  18]\n",
    " [ 51 102 107  43]\n",
    " [ 23  70 148  27]\n",
    " [  4   5   8   3]]```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
